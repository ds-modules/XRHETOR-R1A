{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [XRHETOR-R1A] Module 3: The Rhetoric of Data\n",
    "---\n",
    "<img src=\"https://pixel.nymag.com/imgs/daily/science/2014/10/16/16-trustingraphsnew.nocrop.w536.h2147483647.2x.gif\" style=\"width: 400px; height: 400px;\" />\n",
    "\n",
    "### Professor Amy Tick\n",
    "\n",
    "This module explores how data science can persuade or mislead through intentional or unintentional decisions at every step of the data science process. First, we'll how human judgment still plays a part in seemingly unbiased, 'automated' programming processes by picking apart how Module 2's Wordnet dictionary was compiled. Then, we'll discover how some common cognitive biases are exploited in charts and graphs to emphasize a particular mesage.\n",
    "\n",
    "*Estimated Time: 50 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "### Topics Covered\n",
    "- Cognitive biases\n",
    "- Natural Language Processing\n",
    "- Text Analysis\n",
    "- Data Visualization\n",
    "- Data Communication\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Introduction](#section 0)<br>\n",
    "\n",
    "1 - [Deceitful Data: Three Ways to Make a Dictionary](#section 1)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i - [By Hand](#subsection 1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ii - [By Computer](#subsection 2)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iii - [Both](#subsection 3)\n",
    "\n",
    "\n",
    "2 - [Ambiguous Analysis](#section 2)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i - [Simple Word Counts](#subsection 4)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ii - [Advanced Text Analysis](#subsection 5)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; iii - [Statistical Significance](#subsection 6)\n",
    "\n",
    "3 - [Grifting Graphs](#section 3)<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i - [Types of Graphs](#subsection 7)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ii - [Color](#subsection 8)\n",
    "\n",
    "4 - [What's Next?](#section 4)<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy as sp\n",
    "!pip install plotly\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction <a id='section 0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  *“No study is less alluring or more dry and tedious than statistics, unless the mind and imagination are set to work.” - William Playfair, inventor of the line graph, bar graph, and pie chart. *\n",
    "\n",
    "As data science becomes more and more in-demand, it has emerged as a powerful rhetorical tool. Major news sources pair their stories with 'infographics,' while [studies]('http://journals.sagepub.com/doi/abs/10.1177/0963662514549688') [show]('http://lsr.nellco.org/cgi/viewcontent.cgi?article=1476&context=nyu_plltwp') that the average person finds data and data visualizations highly persuasive. After all, 'numbers don't lie'. Or do they?\n",
    "\n",
    "Let's return to the data analysis we did in module 2 and see, at each step of the process, the many opportunities to make our numbers 'lie.' As a refresher, here's a map of the data science process we used.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/Data_visualization_process_v1.png\" style=\"width: 550px; height: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Deceitful Data: Three Ways to Make a Dictionary <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science starts with a data set upon which all subsequent analysis is built. If that data is skewed, incomplete, or just plain wrong, it's impossible to draw accurate conclusions from it.\n",
    "\n",
    "In module 02, we relied on the set of Moral Foundations words and their synonyms, collected in a Python dictionary, to answer questions about candidate and party values. Let's look further into the ways such data sets are constructed and how they can lead you horribly astray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Method 1: Do it by hand*<a id='subsection 1'></a>\n",
    "\\**or by your grad students' hands*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original MFT word count analysis was done on religious sermon texts by Graham, Haidt, and Nosek as detailed in [this paper](http://projectimplicit.net/nosek/papers/GHN2009.pdf). Their methodology for constructing their dictionary is below:\n",
    "\n",
    "> Dictionary development had an expansive phase and a contractive phase, all occurring before reading the sermons. In the expansive phase Jesse Graham and five research assistants generated as many associations, synonyms, and antonyms for the base foundation words as possible, using thesauruses and conversations with colleagues. This included full words and word stems (for instance, nation  covers national, nationalistic, etc.)...In the contractive phase, Jesse Graham and Jonathan Haidt deleted words that seemed too distantly related to the five foun- dations and also words whose primary meanings were not moral (e.g., just more often means only than fair).\n",
    "\n",
    "The file `haidt_dict.json` contains the relevant portions of the dictionary Graham, Haidt, and Nosek used in their paper. Run the cell below to load the dictionary into the variable `haidt_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the dictionary into a variable\n",
    "with open('../mft_data/haidt_dict.json') as json_data:\n",
    "    haidt_dict = json.load(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling a dictionary this way is extremely time-consuming. Moreover, it involves many, many judgments from researchers, who like all humans are biased. **Selection bias** in data processing leads to a data set that may not be representative of the population to be analyzed. In this case, it could result in a dictionary that includes or omits certain words and leads to skewed word count results. \n",
    "\n",
    "Selection bias can be caused by cognitive biases like **confirmation bias**, where people are predisposed to look for data that they think will confirm their preconceptions.\n",
    "\n",
    "What are some ways selection bias could have happened while making this dictionary? How did Graham, Haidt, and Nosek try to avoid biasing their data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Method 2: Write some code<a id='subsection 2'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to make a dictionary much faster by writing code that utilizes Wordnet. \n",
    "\n",
    "[Wordnet](https://wordnet.princeton.edu/) is a database of words and meanings that can be used like a sophisticated thesaurus. Each word is associated with one or more senses (meanings), and each sense is associated with a **synset**- a set of synomyms associated with that meaning. \n",
    "\n",
    "Wordnet functions can be used by typing `wn.` (which tells Python we want to use Wordnet) followed by the function. Try looking up a set of Synsets by calling the `synsets` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up synsets by calling wn.synsets()\n",
    "# Hint: the synsets function takes one argument: a word in the form of a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synset contains a lot of information.\n",
    "\n",
    "The function `lemma_names()` returns a list of synonyms for that sense.\n",
    "\n",
    "`definition()` returns the definition.\n",
    "\n",
    "`example()` gives an example of the word being used in a sentence. \n",
    "\n",
    "Try calling these functions on the `data_synset` synset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The synset for the first sense of 'data'\n",
    "data_synset = wn.synset('data.n.01')\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lemma_names()` function suggests a way to make our Moral Foundations dictionary. For each Moral Foundation:\n",
    "\n",
    "1. Look up the foundation word in Wordnet to get a list of synsets\n",
    "2. For each synset, use `lemma_names()` to get a lit of synonyms\n",
    "\n",
    "The function `get_entries` does these two steps. `get_entries` uses some Python syntax we haven't learned yet; use the docstring and the comments to get a sense of what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define the get_rel_wds function\n",
    "\n",
    "def get_rel_wds(word):\n",
    "    ''' returns the set of synonyms and antonyms for WORD in Wordnet'''\n",
    "    # get a list of synsets\n",
    "    syns = wn.synsets(word)\n",
    "    # the list to return\n",
    "    values = []\n",
    "    # cycle through each synset\n",
    "    for syn in syns:\n",
    "        # cycle through words\n",
    "        for lem in syn.lemmas():\n",
    "            # check for duplicates and add to values\n",
    "            if lem.name() not in values:\n",
    "                values.append(lem.name())\n",
    "            #check for antonyms and add to values\n",
    "            if lem.antonyms():\n",
    "                for ant in lem.antonyms():\n",
    "                    if ant.name() not in values:\n",
    "                        values.append(ant.name())     \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using `get_rel_wds` for each of the Moral Foundations: \"care\", \"fairness\", \"loyalty\", \"authority\",  and \"sanctity\". What entries do you get back? What do you notice about the different entries?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_rel_wds to get synonym lists\n",
    "get_rel_wds('care')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the entries from Wordnet to those in `haidt_dict`. Which entries or kinds of entries are similar? Which are different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, you can access the entries for a key by putting the key in square brackets []\n",
    "# You can get the keys for this dictionary using .keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Method 3: Write some code and do some by hand <a id='subsection 3'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By coding, we can create a dictionary in a fraction of the time it would take by hand. But, the dictionary is far from perfect: \n",
    "\n",
    "* the computer doesn't know the difference between the senses we want and the senses we don't, so some entries are unrelated to the foundation\n",
    "* some words that seem like obvious entries aren't included\n",
    "* the number of entries that are returned for each foundation are very different. Foundation words that aren't as commonly used (like 'sanctity') have very few entries.\n",
    "\n",
    "Ultimately, we created the dictionary used in Module 02 by combining methods 1 and 2: writing code to get synonyms, then adding and subtracting additional synonyms using our judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Ambiguous Analysis <a id='section 2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is collected and processed, the next step is to do exploratory analysis, then model and estimate. Here, we'll evaluate the approach we used in Module 02 as well as some more advanced text-analysis methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Simple word counts <a id='subsection 4'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word count metric is appealing: it's straightforward to code, fast to calculate, and easy to visualize. However, it relies heavily on the dictionary it uses, making it very susceptible to selection bias.\n",
    "\n",
    "The cell below compares the averages for the speech data, calculated once using the WordNet dictionary from Module 02, and recalculated using the dictionary from the Graham, Haidt, Nosek study. How large is the difference between the two sets of averages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the table of averages\n",
    "foundation_avg = pd.read_csv('party_avgs.csv', index_col = 0)\n",
    "haidt_foundation_avg = pd.read_csv('haidt_party_avgs.csv', index_col = 0)\n",
    "\n",
    "# plot the averages\n",
    "foundation_avg.plot.barh(color=['b', 'r'])\n",
    "plt.title('Wordnet Dictionary')\n",
    "\n",
    "haidt_foundation_avg.plot.barh(color=['b', 'r'])\n",
    "plt.title('Haidt Dictionary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue with this approach is **word ambiguity**. One of our entries for 'care' is 'charg', as in \"the babysitter is in charge of taking care of the baby.\" But, Python returns matches for these uses of the word in speeches:\n",
    "\n",
    "* [20]\"*...a unique advisory group charged with thinking of new ideas...*\" Yes!\n",
    "* [86]\"*...so much in line with what often we are charged with...*\" Yes!\n",
    "* [169]\"*...it is even worse that you re being charged interest rates..*\" No?\n",
    "* [39]\"*...imagine if you could charge your car in your garage at night...*\" Definitely no.\n",
    "\n",
    "Clearly, Python's string-matching function is not sophisticated enough to pick out only the word senses we intend. \n",
    "\n",
    "Some more sophisticated algorithms can try to infer ambigous word meaning from a word's context ([Madly Ambiguous](http://madlyambiguous.osu.edu:1035/) uses Wordnet for this very purpose!). However, even they are far from 100% accurate. The field of **Natural Language Processing** has been progressing by leaps and bounds to improve computers' ability to process language in the way that humans do naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Advanced Text Analysis <a id='subsection 5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given how biased we know human brains to be, it's tempting to remove the human element from analysis as much as possible. The **Bag of Words** method is one way to analyze texts by their word counts, *without* making decisions about which words are of interest.\n",
    "\n",
    "*Note: the code in this section uses Python libraries and linear algebra techniques not covered in these modules. You are not expected to write code like this- we are walking you through it so you can see the kinds of things you can do further down the road in data science.*\n",
    "\n",
    "The first step is to combine all the speeches into one big 'word bag', then find the unique set of words across all speeches. Then, a table is made with one column for every unique word in the speeches and one row for every speech (a 'word vector'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All code in this subsection adapted from Deborah Nolan and Joey Gonzalez, DATA-100 Lecture 08. \n",
    "# http://www.ds100.org/fa17/assets/notebooks/08-lec/regular_expression_examples.html\n",
    "\n",
    "# Create a copy of our speech data\n",
    "speech = pd.read_csv('speeches.csv', index_col=0)\n",
    "\n",
    "# create the word vectors\n",
    "vec = TfidfVectorizer()\n",
    "tfidf = vec.fit_transform(speech['clean_speech'])\n",
    "\n",
    "tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line gives the shape of the table: that's a lot of columns, or put another way, that's a very high-dimensional table! It's hard for us to visualize 22316th-dimensional space, but we're good at visualizing 2-dimensional space. So, our next step is to reduce the dimensions of this table, keeping only the most informative dimensions. The code does this by treating the frequency of each individual word as a feature of a speech and finding the feature vectors that vary the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Finds the 2 largest singular values/vectors for the table\n",
    "(u, s, vt) = sp.sparse.linalg.svds(tfidf, k=2)\n",
    "\n",
    "speech['x'] = u[:,0]\n",
    "speech['y'] = u[:,1]\n",
    "\n",
    "# Plot the graph\n",
    "colors = np.array([\"rgba({0},{1},{2},1)\".format(*c) for c in sns.color_palette(\"RdBu_r\", len(speech))])\n",
    "colors[-1] = \"rgba(.99,.5,.2,1.)\"\n",
    "py.iplot([go.Scatter(x = speech['x'], y = speech['y'], mode='markers', marker=dict(color=colors), \n",
    "                     text=speech['Candidate'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph can show underlying patterns to the speech data that may not be visible to the human eye. But, it suffers from two immediate problems:\n",
    "\n",
    "* Difficult to interpret\n",
    "* Doesn't solve word ambiguity issue\n",
    "\n",
    "No perfect solution exists for unbiased text analysis. But, it is still possible to gain powerful, well-supported insights from data by using the right tools for the analysis task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Statistical Significance <a id='subsection 6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if text analyis shows a difference between two groups (such as Democrat and Republican speech), that difference may not be significant. **Statistical significance** is determined by testing hypotheses about how the observed difference came about.\n",
    "\n",
    "There are two hypotheses:\n",
    "\n",
    "1. The **null hypothesis**- Democrats and Republicans use Moral Foundation words at the same rates. Any difference in observed word usage is due to random chance.\n",
    "2. The **alternate hypothesis**- The difference in how Democrats and Republicans use words is not due to chance.\n",
    "\n",
    "By using statistical methods, it's possible to calculate the **p-value**: the chance that we'd see a word usage difference as large or larger than the one we saw in the data, if the null hypothesis were true. For example, a p-value of $0.10$ means there's a ten percent chance that the Democrat-Republican difference we saw would happen if all candidates used words at the same rates. By convention, a p-value of 0.05 is **statistically significant** and a p-value of 0.01 is **highly statistically significant**.\n",
    "\n",
    "But, significance can be manufactured using **p-hacking**: manipulating test data without a hypothesis to find patterns, then presenting those patterns as significant.\n",
    "\n",
    "Go to the [FiveThirtyEight Interactive](https://projects.fivethirtyeight.com/p-hacking/) to do some p-hacking of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Grifting Graphs <a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once analysis is complete, a proud data scientist will want to communicate their results. A good visualization can draw attention to a pattern, summarize a finding, or support an argument. This section will cover guidelines for displaying data faithfully and convincingly.\n",
    "\n",
    "To show how the same data can be represented many different ways, we'll use the speech data from Module 02- specifically, the average Moral Foundation percents we calculated. Run the cell below to load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the speech data into a pandas table\n",
    "foundation_avg = pd.read_csv('party_avgs.csv', index_col=0).transpose()\n",
    "foundation_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Types of Graphs <a id='subsection 7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of plots are great for emphasizing different things. Pandas makes plotting easy: for a DataFrame with numerical data like `foundation_avg`, you can create a plot by calling `.plot` followed by the specific plotting method. For example, in Module 02 we relied heavily on bar graphs using `.bar()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of foundation averages. \n",
    "# Note: the figsize argument is optional, but can make the plot bigger and easier to read\n",
    "foundation_avg.plot.bar(figsize=(9,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plots are great for this purpose because they make it very easy to compare numerical variables to one another. In this case, we can easily compare the average percentages of each foundation for each party.\n",
    "\n",
    "But, not all bar plots are created equal. Here, it's easy to compare percentages within a party, but harder to compare percentages across parties because Democratic and Republican averages are so far apart. \n",
    "\n",
    "We can emphasize the differences between parties for each foundation by **transposing** `foundation_avg` (i.e. making the columns into rows and the rows into columns) and making a bar chart of that. Create a variable `party_avg` by calling `.transpose()` on `foundtion_avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "party_avg = foundation_avg.transpose()\n",
    "party_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create another bar chart for `party_avg`, which has the same data as the `foundation_avg` bar plot but different bar groupings. When would you want to use this plot instead of the first one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "party_avg.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to the bar plot is the **stacked bar plot**, which stacks all of the bars for each party on top of each other. Create a stacked bar plot for `foundation_avg` below by creating a bar plot, then adding the argument `stacked=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacked bar plot of foundation averages. \n",
    "# Hint: this should look like the code for a bar graph, but with one extra argument for bar()\n",
    "foundation_avg.plot.bar(stacked=True, figsize=(11,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked plots combine a lot of info into fewer bars, which can be visually appealing. But, they can cause comparison errors because they **jiggle the baseline**: for every stacked bar except the base bar, the baseline begins at a different height, making it hard to compare lengths.\n",
    "\n",
    "Here's an extreme example of jiggling (with a few other bonus graph sins: which ones can you find?):\n",
    "\n",
    "<img src=\"https://i.kinja-img.com/gawker-media/image/upload/s--DstspxbR--/c_fit,fl_progressive,q_80,w_636/b5icpqdsnzsdaezvnrm1.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Another way we might represent our percentages is with a **pie plot**, where each variable value is represented by a 'slice' of a circle. Create a pie chart using `.pie()` for the `party_avg_pie` table. Note: since there are 2 parties, pandas needs to be told to make one pie plot for each party by including the argument `subplot=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this table to make a pie chart. \n",
    "# Values are multiplied to be larger so pandas plots it more easily (the proportions are the same)\n",
    "party_avg_pie = party_avg * 1000\n",
    "\n",
    "# Create a pie chart using .pie()\n",
    "# Hint: don't forget to set subplot=True!\n",
    "# Optional: set legend=False and figsize=(9,5) to get a cleaner graph\n",
    "party_avg_pie.plot.pie(subplots=True, legend=False, figsize=(9,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie charts are visually appealing, but they can be problematic for readers. Consider these two bars:\n",
    "\n",
    "<img src='len_compare.png' width=600 height=200>\n",
    "\n",
    "It's quick and easy to see that the top one is twice as long as the bottom. (That's why we like bar plots so much)\n",
    "\n",
    "Now, consider this image. How much smaller is the second pizza compared to the first?\n",
    "\n",
    "<img src='area_compare.png' width=500 height=150>\n",
    "\n",
    "The first one has about twice the area of the second, but it's much harder to form a confident and accurate judgment.\n",
    "\n",
    "This has major implications for pie charts and other area-based visualizations. While representing numbers with area seems intuitive, adding the extra dimension makes it harder for people to form accurate comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Color <a id='subsection 8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human visual cortex processes color preattentively; that is, people notice colors without trying to. Therefore, color can be a powerful tool to communicate information, and its use in data visualization has been the focus of many [studies](https://conference.scipy.org/scipy2014/schedule/presentation/1741/).\n",
    "\n",
    "Pandas allows users to change their plot colors by including the `color=` argument. `color` can be set to equal a single color or a list of colors- one for each type of bar in the legend.\n",
    "\n",
    "Try changing the colors for a bar plot of `party_avg`. You'll need a list of two colors, one for Democrats and one for Republicans in that order. Here are some of the colors pandas recognizes:\n",
    "\n",
    "| Color   | code |\n",
    "|---------|------|\n",
    "| blue    | 'b'  |\n",
    "| red     | 'r'  |\n",
    "| green   | 'g'  |\n",
    "| magenta | 'm'  |\n",
    "| yellow  | 'y'  |\n",
    "| cyan    | 'c'  |\n",
    "| black   | 'k'  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar graph with non-default colors with the color= argument\n",
    "party_avg.plot.bar(color=['m', 'k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas plot functions can also take an argument called `colormap=` that colors graphs from a premade pallete. \n",
    "\n",
    "Try creating a bar plot for `foundation_avg` where `colormap` is set to equal `Pastel2`. You can also try out any of the other colormaps listed [here](https://scipy.github.io/old-wiki/pages/Cookbook/Matplotlib/Show_colormaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for foundation_avg where bar() has the argument colormap='Pastel2'\n",
    "foundation_avg.plot.bar(colormap='Pastel2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colors help to create contrast and facilitate comparison. If colors are too similar to one another, two separate bars start to look like one individual blob and it's hard to compare the values. Set the colormap for `foundation_avg`'s bar plot equal to 'spring' to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for foundation_avg with the 'spring' colormap\n",
    "foundation_avg.plot.bar(colormap='spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, colors can communicate information about the importance of data. Lighter colors usually make areas look larger than darker colors. Furthermore, darker colors stand out more against the white background of most presentation platforms (paper, slideshows, Jupyter notebooks) and tend to look more prominent, weighty, and important.\n",
    "\n",
    "Using a colormap that ranges from light to dark, then, can imply that your data varies along a scale, and the data at the dark end is more important. \n",
    "\n",
    "Try setting the colormap for a `foundation_avg` bar plot to `'Blues'`. How does that change our perception of the information? Is it appropriate for this graph, and if not, can you think of a graph where it would be appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for foundation_avg with the 'Blues' colormap\n",
    "foundation_avg.plot.bar(colormap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing a plot's color palatte can also improve understanding by matching a **mental model**. For instance, Democrats are associated with the color blue while Republicans are associated with red. If a plot violates these associations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to make a very misleading plot\n",
    "party_avg.plot.bar(color=['r', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...a reader could very easily read the results as exactly opposite what they are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. What's Next? <a id='section 4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science is a fast-growing field with applications in almost every subject you can imagine. Students and researchers alike have used Jupyter notebooks and data-driven methods to do everything from completing a lower-division class problem set to presenting a graduate research project. \n",
    "\n",
    "If you'd like to learn more about how to incorporate data science into your academic career:\n",
    "\n",
    "* [DATA-8](http://data8.org) is offered every semester and is a great introduction to coding and statistics. The website includes links to the textbook, syllibi, and past homeworks.\n",
    "* Data Science [Connector Courses](https://data.berkeley.edu/education/connectors) teach applied data science in everything from literature to cancer research. They can be taken with or after DATA-8.\n",
    "* The Berkeley Institute for Data Science ([BIDS](https://bids.berkeley.edu/)) hosts data science talks, research resources organized by field, and office hours for those interested in more in-depth data science research.\n",
    "* [DLAB](http://dlab.berkeley.edu/) also offers workshops and consulting to help you hone your skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Playfair, W. (1801). The Statistical Breviary: Shewing, on a Principle Entirely New, the Resources of Every State and Kingdom in Europe; Illustrated with Stained Copper-plate Charts the Physical Powers of Each Distinct Nation with Ease and Perspicuity: to which is Added, a Similar Exhibition of the Ruling Powers of Hindoostan. T. Bensley, Bolt Court, Fleet Street."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto, Sean Seungwoo Son, Sujude Dalieh\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
