{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Web Scraping Political Speeches from the 2016 US Presidential Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_url = \"http://www.presidency.ucsb.edu/2016_election.php\"\n",
    "base_url = \"http://www.presidency.ucsb.edu/\"\n",
    "response = requests.get(source_url)\n",
    "soup = bs(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_text(bs_expression):\n",
    "    out = []\n",
    "    for elem in bs_expression:\n",
    "        out.append(elem.text)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specifies which candidates we want to scrape (by indicating their row index)\n",
    "#these are the indices for all Republicans\n",
    "candidate_rows = [soup.select(\"td.doctext\")[i] for i in [j * 2 + 10 for j in range(16)]]\n",
    "\n",
    "#for dems, uncomment the line below\n",
    "# candidate_rows = [soup.select(\"td.doctext\")[i] for i in [j * 2 for j in range(5)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gets names and last names of all candidates\n",
    "names = [to_text(cand.find_all(\"span\"))[0] for cand in candidate_rows]\n",
    "lastnames = [name.split(\" \")[1] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find links to each candidate's list of speeches, statments, and press releases\n",
    "link_of_sources = [c.find_all(\"a\") for c in candidate_rows]\n",
    "link_of_campaign_speeches = [l[0]['href'] for l in link_of_sources]\n",
    "link_of_statements = [l[1]['href'] for l in link_of_sources]\n",
    "link_of_press_releases = [l[2]['href'] for l in link_of_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate urls\n",
    "campaign_speeches_url = [base_url + l for l in link_of_campaign_speeches]\n",
    "statements_url = [base_url + l for l in link_of_statements]\n",
    "press_releases_url = [base_url + l for l in link_of_press_releases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_campaign_speeches = [requests.get(url) for url in campaign_speeches_url]\n",
    "response_statements = [requests.get(url) for url in statements_url]\n",
    "response_press_releases = [requests.get(url) for url in press_releases_url]\n",
    "\n",
    "#html of candidate pages listing individual speeches\n",
    "soup_campaign_speeches = [bs(response.text, \"html.parser\") for response in response_campaign_speeches]\n",
    "soup_statements = [bs(response.text, \"html.parser\") for response in response_statements]\n",
    "soup_press_releases = [bs(response.text, \"html.parser\") for response in response_press_releases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finds links to individual speeches - use indices 49 through (len - 5)\n",
    "speech_url = [[base_url + speeches.select(\"tr\")[i].a['href'].split(\"..\")[1] for i in range(49, len(speeches.select(\"tr\")) - 4)] for speeches in soup_campaign_speeches]\n",
    "statements_url = [[base_url + speeches.select(\"tr\")[i].a['href'].split(\"..\")[1] for i in range(49, len(speeches.select(\"tr\")) - 4)] for speeches in soup_statements]\n",
    "press_url = [[base_url + speeches.select(\"tr\")[i].a['href'].split(\"..\")[1] for i in range(49, len(speeches.select(\"tr\")) - 4)] for speeches in soup_press_releases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_speech = [[requests.get(s) for s in c] for c in speech_url]\n",
    "response_statement = [[requests.get(s) for s in c] for c in statements_url]\n",
    "response_press = [[requests.get(s) for s in c] for c in press_url]\n",
    "\n",
    "#html of individual speeches\n",
    "soup_speech = [[bs(response.text, \"html.parser\") for response in c] for c in response_speech]\n",
    "soup_statement = [[bs(response.text, \"html.parser\") for response in c] for c in response_statement]\n",
    "soup_press = [[bs(response.text, \"html.parser\") for response in c] for c in response_press]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find titles of each text\n",
    "speech_titles = [[to_text(s.findAll(\"span\", {\"class\": \"paperstitle\"}))[0] for s in c] for c in soup_speech]\n",
    "statement_titles = [[to_text(s.findAll(\"span\", {\"class\": \"paperstitle\"}))[0] for s in c] for c in soup_statement]\n",
    "press_titles = [[to_text(s.findAll(\"span\", {\"class\": \"paperstitle\"}))[0] for s in c] for c in soup_press]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find text of each text\n",
    "speech_text = [[to_text(s.findAll(\"span\", {\"class\": \"displaytext\"}))[0] for s in c] for c in soup_speech]\n",
    "statement_text = [[to_text(s.findAll(\"span\", {\"class\": \"displaytext\"}))[0] for s in c] for c in soup_statement]\n",
    "press_text = [[to_text(s.findAll(\"span\", {\"class\": \"displaytext\"}))[0] for s in c] for c in soup_press]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find date of each text\n",
    "speech_date = [[to_text(s.findAll(\"span\", {\"class\": \"docdate\"}))[0] for s in c] for c in soup_speech]\n",
    "statement_date = [[to_text(s.findAll(\"span\", {\"class\": \"docdate\"}))[0] for s in c] for c in soup_statement]\n",
    "press_date = [[to_text(s.findAll(\"span\", {\"class\": \"docdate\"}))[0] for s in c] for c in soup_press]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove non-ASCII characters\n",
    "clean_speech_text = [[s.encode('ascii',errors='ignore') for s in c] for c in speech_text]\n",
    "clean_statement_text = [[s.encode('ascii',errors='ignore') for s in c] for c in statement_text]\n",
    "clean_press_text = [[s.encode('ascii',errors='ignore') for s in c] for c in press_text]\n",
    "\n",
    "clean_speech_titles = [[s.encode('ascii',errors='ignore') for s in c] for c in speech_titles]\n",
    "clean_statement_titles = [[s.encode('ascii',errors='ignore') for s in c] for c in statement_titles]\n",
    "clean_press_titles = [[s.encode('ascii',errors='ignore') for s in c] for c in press_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function takes in an index (which row are they on the start page?), a type ('c', 's', or 'p')\n",
    "#and three lists of lists that we've generated\n",
    "def generate_csv(index, type, title, text, date):\n",
    "    header = ['Candidate', 'Party', 'Type', 'Date', 'Title', 'Speech']\n",
    "    with open('csv/' + lastnames[index] + '_' + type + '.csv', 'w') as output_file:\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow(header)\n",
    "        for i in range(len(title[index])):\n",
    "            row = [names[index], 'R', type, date[index][i], title[index][i], text[index][i]]\n",
    "            csv_writer.writerow(row)\n",
    "    pandas.read_csv('csv/' + lastnames[index] + '_' + type + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each candidate in names, generate three csv files\n",
    "for i in range(len(names)):\n",
    "    generate_csv(i, 'c', clean_speech_titles, clean_speech_text, speech_date)\n",
    "    generate_csv(i, 's', clean_statement_titles, clean_statement_text, statement_date)\n",
    "    generate_csv(i, 'p', clean_press_titles, clean_press_text, press_date)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
